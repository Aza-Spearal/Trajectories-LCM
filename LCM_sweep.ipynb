{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ndef nan_clip_grad(model):\\n    for param in model.named_parameters():\\n        if param.grad is not None:\\n            if torch.isnan(param.grad).any():\\n                torch.nn.utils.clip_grad_norm_(model.parameters(), 1)\\n\\n    \\ndef check_for_nans(model):\\n    for name, param in model.named_parameters():\\n        for attr_name, attr_value in [(\"weights\", param.data), (\"grad\", param.grad)]:\\n            if param.grad is not None:\\n                if torch.isnan(attr_value).any():\\n                    print(f\"NaN detected in {attr_name} of {name}\")\\n                    print(\"Stopping training\")\\n                    return True\\n    return False\\n    \\n\\ndef check_for_nans(model):\\n    for name, param in model.named_parameters():\\n        if param.grad is not None:\\n            if torch.isnan(param.grad).any():\\n                print(f\"NaN detected in weights of {name}\")\\n                print(\"Stopping training\")\\n                #tune.report({\\'Cossim\\': 10, \\'MSE\\': 10})\\n                wandb.log({\\'Cossim\\': 10, \\'MSE\\': 10})\\n                return True\\n    return False\\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.optim import AdamW, SGD\n",
    "from torch.optim.lr_scheduler import CosineAnnealingWarmRestarts\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "import gc\n",
    "import numpy as np\n",
    "import os\n",
    "import itertools\n",
    "import math\n",
    "\n",
    "from sonar.inference_pipelines.text import EmbeddingToTextModelPipeline\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "import wandb\n",
    "import traceback\n",
    "\n",
    "from transformers.utils import logging\n",
    "logging.set_verbosity_error()\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "d_emb = 1024\n",
    "\n",
    "pad = torch.from_numpy(np.load('/workspace/eloise/sentemb/data/pad.npy')).to(device)\n",
    "\n",
    "cos = nn.CosineSimilarity(dim=-1)\n",
    "cossim = nn.CosineEmbeddingLoss()\n",
    "mse = nn.MSELoss()\n",
    "\n",
    "def criterion(output, target):\n",
    "    \n",
    "    mask = ~(target == pad).all(dim=-1) #[batch, seq]\n",
    "    target = target[mask]\n",
    "    output = output[mask]\n",
    "    \n",
    "    cossim_ = cossim(output, target, torch.full((1,), 1).to(device))\n",
    "    mse_ = mse(output, target)\n",
    "    return [cossim_, mse_]\n",
    "\n",
    "'''\n",
    "def nan_clip_grad(model):\n",
    "    for param in model.named_parameters():\n",
    "        if param.grad is not None:\n",
    "            if torch.isnan(param.grad).any():\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), 1)\n",
    "\n",
    "    \n",
    "def check_for_nans(model):\n",
    "    for name, param in model.named_parameters():\n",
    "        for attr_name, attr_value in [(\"weights\", param.data), (\"grad\", param.grad)]:\n",
    "            if param.grad is not None:\n",
    "                if torch.isnan(attr_value).any():\n",
    "                    print(f\"NaN detected in {attr_name} of {name}\")\n",
    "                    print(\"Stopping training\")\n",
    "                    return True\n",
    "    return False\n",
    "    \n",
    "\n",
    "def check_for_nans(model):\n",
    "    for name, param in model.named_parameters():\n",
    "        if param.grad is not None:\n",
    "            if torch.isnan(param.grad).any():\n",
    "                print(f\"NaN detected in weights of {name}\")\n",
    "                print(\"Stopping training\")\n",
    "                #tune.report({'Cossim': 10, 'MSE': 10})\n",
    "                wandb.log({'Cossim': 10, 'MSE': 10})\n",
    "                return True\n",
    "    return False\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validation(epoch, cpt, model, valloader, val_list):\n",
    "    cossim_loss = 0; mse_loss = 0\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for src in valloader:\n",
    "            src = src.to(device)\n",
    "            outputs = model.predict_next_sentence(src[:, :-1])\n",
    "            [cossim_, mse_] = criterion(outputs, src[:, 1:])\n",
    "            cossim_loss += cossim_.item(); mse_loss += mse_.item()\n",
    "        cossim_loss = cossim_loss/len(valloader)\n",
    "        mse_loss = mse_loss/len(valloader)\n",
    "        val_list.append([cossim_loss, mse_loss])\n",
    "        print('Epoch', epoch+1, 'Part', cpt, \"Cossim\", cossim_loss, \"MSE\", mse_loss)\n",
    "        #tune.report({\"Cossim\": cossim_loss, \"MSE\": mse_loss})\n",
    "        wandb.log({'Cossim': cossim_loss, 'MSE': mse_loss})\n",
    "    model.train()\n",
    "    return val_list, cpt+1\n",
    "\n",
    "\n",
    "def autoregr_infer(model, prompts):\n",
    "    seq = 20-1\n",
    "    list_autoregr = []\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        prompts = prompts.unsqueeze(0)\n",
    "        for src in prompts:\n",
    "            src = src.to(device)\n",
    "            #autoregr = torch.cat((src, model(src)), dim=1)\n",
    "            autoregr = torch.cat((src, model.predict_next_sentence(src)), dim=1)\n",
    "            for i in range(seq-1):\n",
    "                #outputs = model(autoregr)[:, -1].unsqueeze(1)\n",
    "                outputs = model.predict_next_sentence(autoregr)[:, -1].unsqueeze(1)\n",
    "                autoregr = torch.cat((autoregr, outputs), dim=1)\n",
    "            list_autoregr.append(autoregr[:, 1:])\n",
    "    return torch.cat(list_autoregr, dim=0)\n",
    "\n",
    "\n",
    "def calculate_score(output, targets):\n",
    "    seq = 20-1\n",
    "\n",
    "    score_one_sum = 0\n",
    "    score_sum_pad = torch.zeros(seq).to(device)\n",
    "    pad_nbr_sum = torch.zeros(seq).to(device)\n",
    "\n",
    "    for batch, target in enumerate(targets):\n",
    "        out = output[batch][:len(target)].to(device)\n",
    "        \n",
    "        score = cos(out, target)\n",
    "\n",
    "        score_one_sum += score.mean()\n",
    "        score_sum_pad += F.pad(score, (0, seq - len(score)))\n",
    "        pad_nbr_sum += F.pad(torch.ones(len(score)), (0, seq - len(score))).to(device)\n",
    "\n",
    "    paragraphed_score = score_sum_pad/pad_nbr_sum\n",
    "    paragraphed_score = [round(elem.item(), 2) for elem in paragraphed_score]\n",
    "    final_score = score_one_sum.item()/len(targets)\n",
    "    return final_score, paragraphed_score\n",
    "\n",
    "\n",
    "def test(model, test_data, config):\n",
    "    \n",
    "    sonarprompt, sonaroutput, jasperoutput = test_data\n",
    "    \n",
    "    output_autoregr = autoregr_infer(model, sonarprompt)\n",
    "    final_sonar, paragraphed_sonar = calculate_score(output_autoregr, sonaroutput)\n",
    "\n",
    "    print('Sonar score:', final_sonar, paragraphed_sonar)\n",
    "    wandb.log({'Final sonar': final_sonar, 'Paragraphed sonar': paragraphed_sonar})\n",
    "\n",
    "    vec2text_model = EmbeddingToTextModelPipeline(decoder=\"text_sonar_basic_decoder\", tokenizer=\"text_sonar_basic_decoder\", device=device)\n",
    "\n",
    "    text_autoregr = []\n",
    "    for data in output_autoregr[:10]:\n",
    "        text_autoregr.append(vec2text_model.predict(data, target_lang=\"eng_Latn\", max_seq_len=64))\n",
    "    \n",
    "    torch.cuda.empty_cache(); del vec2text_model, sonarprompt, sonaroutput, output_autoregr; gc.collect()\n",
    "\n",
    "\n",
    "    jasper = SentenceTransformer(\"infgrad/jasper_en_vision_language_v1\",\n",
    "        trust_remote_code=True,\n",
    "        device=device,\n",
    "        model_kwargs={\"torch_dtype\":  torch.bfloat16 if device == torch.device('cuda') else torch.float32},\n",
    "    ); jasper.max_seq_length = 1024\n",
    "\n",
    "    jasper_emb = []\n",
    "    for data in text_autoregr:\n",
    "        jasper_emb.append(torch.from_numpy(jasper.encode(data)).to(device))\n",
    "\n",
    "    torch.cuda.empty_cache(); del jasper; gc.collect()\n",
    "    \n",
    "    \n",
    "    final_jasper, paragraphed_jasper = calculate_score(jasper_emb, jasperoutput)\n",
    "    \n",
    "    torch.cuda.empty_cache(); del jasper_emb, jasperoutput; gc.collect()\n",
    "\n",
    "    if final_sonar > 0.4 or final_jasper > 0.4:\n",
    "        #torch.save(model.state_dict(), '/workspace/eloise/sentemb/Base_LCM_' + str(round(final_sonar, 2)) + '_' + str(round(final_jasper, 2)) + '_' + str(layers) + '-' + str(heads) + '-' + str(hidden_dim)+ '.pth')\n",
    "        #torch.save([model.state_dict(), config], '/workspace/eloise/sentemb/Base_LCM_' + str(round(final_sonar, 2)) + '_' + str(round(final_jasper, 2)) + '.pth')\n",
    "        torch.save([model.state_dict(), config], '/workspace/eloise/sentemb/Diff_LCM_' + str(round(final_sonar, 2)) + '_' + str(round(final_jasper, 2)) +'_'+str(config['model_dim'])+'_'+str(config['model_arch'][0])+'_'+str(config['model_arch'][1]) + '.pth')\n",
    "\n",
    "    print('Jasper score:', final_jasper, paragraphed_jasper)\n",
    "    wandb.log({'Final jasper': final_jasper, 'Paragraphed jasper': paragraphed_jasper})\n",
    "    \n",
    "    print('Final score:', (final_sonar+final_jasper)/2)\n",
    "    wandb.log({'Final score': (final_sonar+final_jasper)/2})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lcm.models.base_lcm.archs import toy_base_lcm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from lcm.models.two_tower_diffusion_lcm.archs import toy_two_tower_diffusion_lcm\n",
    "from lcm.models.two_tower_diffusion_lcm.archs import toy_lcm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lcm.models.two_tower_diffusion_lcm.archs import toy_lcm\n",
    "from lcm.models.two_tower_diffusion_lcm.builder import create_two_tower_diffusion_lcm_model\n",
    "\n",
    "model = create_two_tower_diffusion_lcm_model(toy_lcm())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lcm.models.base_lcm.archs import toy_base_lcm, base_lcm_1_6B, base_lcm_tuner, base_lcm_max\n",
    "from lcm.models.base_lcm.builder import BaseLCModelBuilder\n",
    "\n",
    "model = BaseLCModelBuilder(toy_base_lcm()).build_model('lolo')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lcm.models.two_tower_diffusion_lcm.archs import toy_lcm\n",
    "from lcm.models.two_tower_diffusion_lcm.builder import create_two_tower_diffusion_lcm_model\n",
    "\n",
    "def objective(config, data):\n",
    "    \n",
    "    traindata, valloader, test_data = data\n",
    "    \n",
    "    #model = BaseLCModelBuilder(base_lcm_tuner(config), device=device).build_model(config.sonar_normalizer_name)\n",
    "    #model = BaseLCModelBuilder(base_lcm_max(config), device=device).build_model(config.sonar_normalizer_name)\n",
    "    #model = BaseLCModelBuilder(base_lcm_max(config), device=device).build_model()\n",
    "    model = create_two_tower_diffusion_lcm_model(toy_lcm())\n",
    "    model_save = model\n",
    "    \n",
    "    optimizer = AdamW(model.parameters(), lr=config.lr, weight_decay=config.weight_decay)\n",
    "\n",
    "    if config.scheduler:\n",
    "        scheduler = CosineAnnealingWarmRestarts(optimizer, T_0=config.T_0)\n",
    "    else:\n",
    "        scheduler = None\n",
    "\n",
    "    val_list=[]\n",
    "    cpt = 1\n",
    "    loss_type=1 #COSSIM:0   MSE:1\n",
    "    patience = 3\n",
    "\n",
    "    checker = 6400/config.batch_size\n",
    "\n",
    "    print('')\n",
    "    \n",
    "    for epoch in itertools.cycle(range(10)):\n",
    "        if epoch!=0:\n",
    "                torch.cuda.empty_cache(); del epoch_traindata, trainloader; gc.collect()\n",
    "        if epoch%10==0:\n",
    "            epoch_traindata = traindata\n",
    "        else:\n",
    "            epoch_traindata = torch.from_numpy(np.load('/workspace/eloise/sentemb/data/'+str(epoch)+'00k.npy'))\n",
    "            \n",
    "        trainloader = DataLoader(epoch_traindata, config.batch_size)\n",
    "        \n",
    "        model.train()\n",
    "\n",
    "        for i, src in enumerate(trainloader):\n",
    "            optimizer.zero_grad()\n",
    "            src = src.to(device)\n",
    "            outputs = model.predict_next_sentence(src[:, :-1])\n",
    "            loss = criterion(outputs, src[:, 1:])[loss_type]\n",
    "            loss.backward()\n",
    "            \n",
    "            if config.clip_grad:\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), 1)\n",
    "            \n",
    "            optimizer.step()\n",
    "            \n",
    "            if scheduler is not None:\n",
    "                scheduler.step()\n",
    "            \n",
    "            if i!=0 and i%checker==0:\n",
    "                val_list, cpt = validation(epoch, cpt, model, valloader, val_list)\n",
    "                \n",
    "                if math.isnan(val_list[-1][1]):\n",
    "                    if len(val_list) > 1:\n",
    "                        test(model_save, test_data, config)\n",
    "                    else:\n",
    "                        wandb.log({'Final score': -1})\n",
    "                    break\n",
    "                \n",
    "                #if cpt==3:\n",
    "                #    test(model_save, test_data, config)\n",
    "                \n",
    "                if val_list[0][1]> 0.8: wandb.log({'Cossim': val_list[-1][0], 'MSE': val_list[-1][1]}); test(model_save, test_data, config); break\n",
    "                if cpt==5 and val_list[-1][1]> 0.01: wandb.log({'Cossim': val_list[-1][0], 'MSE': val_list[-1][1]}); test(model_save, test_data, config); break\n",
    "                if cpt==30 and val_list[-1][1]> 5e-5: wandb.log({'Cossim': val_list[-1][0], 'MSE': val_list[-1][1]}); test(model_save, test_data, config); break\n",
    "                \n",
    "                #early stopping code:\n",
    "                if len(val_list) >= 2:\n",
    "                    loss_list = [x[loss_type] for x in val_list]\n",
    "                    if loss_list[-1] < min(loss_list[:-1]):#it performs better, we save the model\n",
    "                        model_save = model\n",
    "                    elif (len(loss_list) - loss_list.index(min(loss_list))) > patience: #no better model in the last epochs\n",
    "                        print('Best: Part', cpt-1-patience, 'Cossim', val_list[-1-patience][0], 'MSE', val_list[-1-patience][1])\n",
    "                        wandb.log({'Cossim': val_list[-1-patience][0], 'MSE': val_list[-1-patience][1]})\n",
    "                        torch.cuda.empty_cache(); del src, outputs, model, optimizer, valloader, trainloader, epoch_traindata, traindata, loss; gc.collect()\n",
    "                        \n",
    "                        #compute Final score\n",
    "                        test(model_save, test_data, config)\n",
    "                        break\n",
    "        else:\n",
    "            continue\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_675054/466776071.py:55: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  test_data = torch.load('data/test_sonarprompt_sonaroutput_jasperoutput.pth')\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Create sweep with ID: ue8adjip\n",
      "Sweep URL: https://wandb.ai/seperability/LCM/sweeps/ue8adjip\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: oftpht28 with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tT_0: 2287\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 16\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tclip_grad: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlcm_ffn_inner_dim: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlcm_num_layers: 18\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlr: 0.0019910084515098784\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmodel_dim: 2048\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscheduler: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tweight_decay: 0.00011578422986358649\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33meloise_benito\u001b[0m (\u001b[33mseperability\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.8"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/workspace/eloise/sentemb/wandb/run-20250722_215034-oftpht28</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/seperability/LCM/runs/oftpht28' target=\"_blank\">grateful-sweep-1</a></strong> to <a href='https://wandb.ai/seperability/LCM' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/seperability/LCM/sweeps/ue8adjip' target=\"_blank\">https://wandb.ai/seperability/LCM/sweeps/ue8adjip</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/seperability/LCM' target=\"_blank\">https://wandb.ai/seperability/LCM</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/seperability/LCM/sweeps/ue8adjip' target=\"_blank\">https://wandb.ai/seperability/LCM/sweeps/ue8adjip</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/seperability/LCM/runs/oftpht28' target=\"_blank\">https://wandb.ai/seperability/LCM/runs/oftpht28</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "TwoTowerDiffusionLCModel.predict_next_sentence() missing 1 required positional argument: 'context'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_675054/466776071.py\", line 64, in main\n",
      "    objective(wandb.config, [traindata, valloader, test_data])\n",
      "  File \"/tmp/ipykernel_675054/1773245199.py\", line 45, in objective\n",
      "    outputs = model.predict_next_sentence(src[:, :-1])\n",
      "  File \"/workspace/eloise/large_concept_model/.venv/lib/python3.10/site-packages/torch/utils/_contextlib.py\", line 116, in decorate_context\n",
      "    return func(*args, **kwargs)\n",
      "TypeError: TwoTowerDiffusionLCModel.predict_next_sentence() missing 1 required positional argument: 'context'\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Final score</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Final score</td><td>-1</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">grateful-sweep-1</strong> at: <a href='https://wandb.ai/seperability/LCM/runs/oftpht28' target=\"_blank\">https://wandb.ai/seperability/LCM/runs/oftpht28</a><br> View project at: <a href='https://wandb.ai/seperability/LCM' target=\"_blank\">https://wandb.ai/seperability/LCM</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20250722_215034-oftpht28/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: 5o2ze9u5 with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tT_0: 4599\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tclip_grad: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlcm_ffn_inner_dim: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlcm_num_layers: 18\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlr: 0.0011845647655743198\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmodel_dim: 2048\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscheduler: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tweight_decay: 0.0024917440909810974\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.8"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/workspace/eloise/sentemb/wandb/run-20250722_215039-5o2ze9u5</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/seperability/LCM/runs/5o2ze9u5' target=\"_blank\">hardy-sweep-2</a></strong> to <a href='https://wandb.ai/seperability/LCM' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/seperability/LCM/sweeps/ue8adjip' target=\"_blank\">https://wandb.ai/seperability/LCM/sweeps/ue8adjip</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/seperability/LCM' target=\"_blank\">https://wandb.ai/seperability/LCM</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/seperability/LCM/sweeps/ue8adjip' target=\"_blank\">https://wandb.ai/seperability/LCM/sweeps/ue8adjip</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/seperability/LCM/runs/5o2ze9u5' target=\"_blank\">https://wandb.ai/seperability/LCM/runs/5o2ze9u5</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "TwoTowerDiffusionLCModel.predict_next_sentence() missing 1 required positional argument: 'context'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_675054/466776071.py\", line 64, in main\n",
      "    objective(wandb.config, [traindata, valloader, test_data])\n",
      "  File \"/tmp/ipykernel_675054/1773245199.py\", line 45, in objective\n",
      "    outputs = model.predict_next_sentence(src[:, :-1])\n",
      "  File \"/workspace/eloise/large_concept_model/.venv/lib/python3.10/site-packages/torch/utils/_contextlib.py\", line 116, in decorate_context\n",
      "    return func(*args, **kwargs)\n",
      "TypeError: TwoTowerDiffusionLCModel.predict_next_sentence() missing 1 required positional argument: 'context'\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Final score</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Final score</td><td>-1</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">hardy-sweep-2</strong> at: <a href='https://wandb.ai/seperability/LCM/runs/5o2ze9u5' target=\"_blank\">https://wandb.ai/seperability/LCM/runs/5o2ze9u5</a><br> View project at: <a href='https://wandb.ai/seperability/LCM' target=\"_blank\">https://wandb.ai/seperability/LCM</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20250722_215039-5o2ze9u5/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Sweep Agent: Waiting for job.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Job received.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: x2pjr8hx with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tT_0: 16953\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 64\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tclip_grad: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlcm_ffn_inner_dim: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlcm_num_layers: 18\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlr: 1.4380294269063304e-05\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmodel_dim: 2048\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tscheduler: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tweight_decay: 1.4288913651147253e-05\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Ctrl + C detected. Stopping sweep.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "message_loop has been closed\n",
      "Traceback (most recent call last):\n",
      "  File \"/workspace/eloise/large_concept_model/.venv/lib/python3.10/site-packages/wandb/sdk/interface/router_sock.py\", line 27, in _read_message\n",
      "    return self._sock_client.read_server_response(timeout=1)\n",
      "  File \"/workspace/eloise/large_concept_model/.venv/lib/python3.10/site-packages/wandb/sdk/lib/sock_client.py\", line 235, in read_server_response\n",
      "    data = self._read_packet_bytes(timeout=timeout)\n",
      "  File \"/workspace/eloise/large_concept_model/.venv/lib/python3.10/site-packages/wandb/sdk/lib/sock_client.py\", line 220, in _read_packet_bytes\n",
      "    raise SockClientClosedError\n",
      "wandb.sdk.lib.sock_client.SockClientClosedError\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/workspace/eloise/large_concept_model/.venv/lib/python3.10/site-packages/wandb/sdk/interface/router.py\", line 56, in message_loop\n",
      "    msg = self._read_message()\n",
      "  File \"/workspace/eloise/large_concept_model/.venv/lib/python3.10/site-packages/wandb/sdk/interface/router_sock.py\", line 29, in _read_message\n",
      "    raise MessageRouterClosedError from e\n",
      "wandb.sdk.interface.router.MessageRouterClosedError\n"
     ]
    }
   ],
   "source": [
    "sweep_conf = {\n",
    "    \"method\": \"bayes\",\n",
    "    \"metric\": {\"goal\": \"maximize\", \"name\": \"Final score\"},\n",
    "    \"parameters\": {\n",
    "        \"batch_size\": {\"values\": [8, 16, 32, 64]},\n",
    "        \"scheduler\": {\"values\": [True, False]},\n",
    "        \"T_0\": {\"min\": 10, \"max\": 20000},\n",
    "        \"lr\": {\"distribution\": \"log_uniform_values\", \"min\": 0.00001, \"max\": 0.1},\n",
    "        \"weight_decay\": {\"distribution\": \"log_uniform_values\", \"min\": 0.00001, \"max\": 0.1},\n",
    "        \"clip_grad\": {\"values\": [True, False]},\n",
    "        \n",
    "        #\"model_dim\": {\"values\": [512, 1024, 2048]},\n",
    "        \"model_dim\": {\"values\": [2048]},\n",
    "        #\"num_attn_heads\": {\"values\": [16]},\n",
    "        \n",
    "        #\"sonar_normalizer_name\": {\"values\": [\"dummy_sonar_normalizer\", \"layernorm\", None]},\n",
    "\n",
    "        #\"frontend_dropout_p\": {\"min\": 0.0, \"max\": 0.5},\n",
    "        #\"frontend_pre_linear_init_fn\": {\"values\": ['xavier', 'sonar', 'zero', 'trunc_normal', 'kaiming_uniform', 'none']},\n",
    "        #\"frontend_scale_embeddings\": {\"values\": [True, False]},\n",
    "        #\"frontend_weight_normalization\": {\"values\": [True, False]},\n",
    "\n",
    "        #\"lcm_final_dropout_p\": {\"min\": 0.0, \"max\": 0.5},\n",
    "        #\"lcm_attention_dropout_p\": {\"min\": 0.0, \"max\": 0.5},\n",
    "        #\"lcm_dropout_p\": {\"min\": 0.0, \"max\": 0.5},\n",
    "        #\"lcm_ffn_inner_dim\": {\"values\": [1, 2, 4]},\n",
    "        \"lcm_ffn_inner_dim\": {\"values\": [2]},\n",
    "        #\"lcm_num_layers\": {\"values\": [2, 8, 16, 24, 32]},\n",
    "        #\"lcm_num_layers\": {\"values\": [2, 8, 14, 24, 32]},\n",
    "        \"lcm_num_layers\": {\"values\": [18]},\n",
    "        #\"lcm_pos_embedding_style\": {\"values\": [\"rope\", \"sine\", \"learned\", \"none\"]},\n",
    "        #\"lcm_use_swiglu\": {\"values\": [True, False]},\n",
    "        #\"lcm_ffn_inner_activation_name\": {\"values\": [\"relu\", \"tanh\", \"elu\", \"leaky_relu\", \"prelu\", \"selu\", \"gelu\", \"silu\", \"softsign\", \"sigmoid\", \"hardsigmoid\", None]},\n",
    "        #\"lcm_ffn_inner_activation_name\": {\"values\": [\"relu\", \"tanh\", \"elu\", \"leaky_relu\", \"selu\", \"gelu\", \"silu\", \"softsign\", \"sigmoid\", \"hardsigmoid\", None]},\n",
    "        #\"lcm_layer_normalization_style\": {\"values\": [\"standard\", \"fp32\", \"rms\", \"unit\"]},\n",
    "        #\"lcm_norm_order_style\": {\"values\": ['pre', 'post', 'normformer']},\n",
    "        #\"lcm_final_norm_order_style\": {\"values\": ['pre', 'post', 'normformer']},\n",
    "        #\"lcm_enable_qk_layernorm\": {\"values\": [True, False]},\n",
    "        #\"lcm_mha_qkv_weight_normalization\": {\"values\": [True, False]},\n",
    "        #\"lcm_mha_output_weight_normalization\": {\"values\": [True, False]},\n",
    "        #\"lcm_mha_output_proj_bias\": {\"values\": [True, False]},\n",
    "        #\"lcm_attention_output_init_fn\": {\"values\": ['xavier', 'sonar', 'zero', 'trunc_normal', 'kaiming_uniform', 'none']},\n",
    "\n",
    "        #\"postnet_dropout_p\": {\"min\": 0.0, \"max\": 0.5},\n",
    "        #\"postnet_linear_init_fn\": {\"values\": ['xavier', 'sonar', 'zero', 'trunc_normal', 'kaiming_uniform', 'none']},\n",
    "        #\"postnet_weight_normalization\": {\"values\": [True, False]},\n",
    "        #\"postnet_layer_normalization_style\": {\"values\": [\"standard\", \"fp32\", \"rms\", \"unit\"]},\n",
    "        #\"postnet_activation_name\": {\"values\": [\"relu\", \"tanh\", \"elu\", \"leaky_relu\", \"prelu\", \"selu\", \"gelu\", \"silu\", \"softsign\", \"sigmoid\", \"hardsigmoid\", None]},\n",
    "    },\n",
    "}\n",
    "\n",
    "np_data = np.load('data/100k_1k_0.npz')\n",
    "traindata = torch.from_numpy(np_data['train'])\n",
    "valloader = DataLoader(torch.from_numpy(np_data['val']), batch_size=64)\n",
    "test_data = torch.load('data/test_sonarprompt_sonaroutput_jasperoutput.pth')\n",
    "torch.cuda.empty_cache(); del np_data; gc.collect()\n",
    "\n",
    "\n",
    "#os.environ[\"WANDB_SILENT\"] = \"true\"\n",
    "\n",
    "def main():\n",
    "    wandb.init()\n",
    "    try:\n",
    "        objective(wandb.config, [traindata, valloader, test_data])\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        traceback.print_exc()\n",
    "        wandb.log({'Final score': -1})\n",
    "    \n",
    "sweep_id = wandb.sweep(sweep_conf, project=\"LCM\")\n",
    "\n",
    "wandb.agent(sweep_id, function=main)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "large-concept-model-py3.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
