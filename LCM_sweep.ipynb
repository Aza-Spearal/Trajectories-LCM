{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.optim import AdamW\n",
    "from torch.optim.lr_scheduler import CosineAnnealingWarmRestarts\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "import gc\n",
    "import numpy as np\n",
    "import itertools\n",
    "import math\n",
    "\n",
    "from sonar.inference_pipelines.text import EmbeddingToTextModelPipeline\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "import wandb\n",
    "import traceback\n",
    "\n",
    "from transformers.utils import logging\n",
    "logging.set_verbosity_error()\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "d_emb = 1024\n",
    "\n",
    "pad = torch.from_numpy(np.load('/LCM/data/pad.npy')).to(device)\n",
    "\n",
    "cos = nn.CosineSimilarity(dim=-1)\n",
    "cossim = nn.CosineEmbeddingLoss()\n",
    "mse = nn.MSELoss()\n",
    "\n",
    "def criterion(output, target):\n",
    "    \n",
    "    mask = ~(target == pad).all(dim=-1) #[batch, seq]\n",
    "    target = target[mask]\n",
    "    output = output[mask]\n",
    "    \n",
    "    cossim_ = cossim(output, target, torch.full((1,), 1).to(device))\n",
    "    mse_ = mse(output, target)\n",
    "    return [cossim_, mse_]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validation(epoch, cpt, model, valloader, val_list):\n",
    "    cossim_loss = 0; mse_loss = 0\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for src in valloader:\n",
    "            src = src.to(device)\n",
    "            outputs = model.predict_next_sentence(src[:, :-1])\n",
    "            [cossim_, mse_] = criterion(outputs, src[:, 1:])\n",
    "            cossim_loss += cossim_.item(); mse_loss += mse_.item()\n",
    "        cossim_loss = cossim_loss/len(valloader)\n",
    "        mse_loss = mse_loss/len(valloader)\n",
    "        val_list.append([cossim_loss, mse_loss])\n",
    "        print('Epoch', epoch+1, 'Part', cpt, \"Cossim\", cossim_loss, \"MSE\", mse_loss)\n",
    "        wandb.log({'Cossim': cossim_loss, 'MSE': mse_loss})\n",
    "    model.train()\n",
    "    return val_list, cpt+1\n",
    "\n",
    "\n",
    "def autoregr_infer(model, prompts):\n",
    "    seq = 20-1\n",
    "    list_autoregr = []\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        prompts = prompts.unsqueeze(0)\n",
    "        for src in prompts:\n",
    "            src = src.to(device)\n",
    "            autoregr = torch.cat((src, model.predict_next_sentence(src)), dim=1)\n",
    "            for i in range(seq-1):\n",
    "                outputs = model.predict_next_sentence(autoregr)[:, -1].unsqueeze(1)\n",
    "                autoregr = torch.cat((autoregr, outputs), dim=1)\n",
    "            list_autoregr.append(autoregr[:, 1:])\n",
    "    return torch.cat(list_autoregr, dim=0)\n",
    "\n",
    "\n",
    "def calculate_score(output, targets):\n",
    "    seq = 20-1\n",
    "\n",
    "    score_one_sum = 0\n",
    "    score_sum_pad = torch.zeros(seq).to(device)\n",
    "    pad_nbr_sum = torch.zeros(seq).to(device)\n",
    "\n",
    "    for batch, target in enumerate(targets):\n",
    "        out = output[batch][:len(target)].to(device)\n",
    "        \n",
    "        score = cos(out, target)\n",
    "\n",
    "        score_one_sum += score.mean()\n",
    "        score_sum_pad += F.pad(score, (0, seq - len(score)))\n",
    "        pad_nbr_sum += F.pad(torch.ones(len(score)), (0, seq - len(score))).to(device)\n",
    "\n",
    "    paragraphed_score = score_sum_pad/pad_nbr_sum\n",
    "    paragraphed_score = [round(elem.item(), 2) for elem in paragraphed_score]\n",
    "    final_score = score_one_sum.item()/len(targets)\n",
    "    return final_score, paragraphed_score\n",
    "\n",
    "\n",
    "def test(model, test_data, config):\n",
    "    \n",
    "    sonarprompt, sonaroutput, jasperoutput = test_data\n",
    "    \n",
    "    output_autoregr = autoregr_infer(model, sonarprompt)\n",
    "    final_sonar, paragraphed_sonar = calculate_score(output_autoregr, sonaroutput)\n",
    "\n",
    "    print('Sonar score:', final_sonar, paragraphed_sonar)\n",
    "    wandb.log({'Final sonar': final_sonar, 'Paragraphed sonar': paragraphed_sonar})\n",
    "\n",
    "    vec2text_model = EmbeddingToTextModelPipeline(decoder=\"text_sonar_basic_decoder\", tokenizer=\"text_sonar_basic_decoder\", device=device)\n",
    "\n",
    "    text_autoregr = []\n",
    "    for data in output_autoregr[:10]:\n",
    "        text_autoregr.append(vec2text_model.predict(data, target_lang=\"eng_Latn\", max_seq_len=64))\n",
    "    \n",
    "    torch.cuda.empty_cache(); del vec2text_model, sonarprompt, sonaroutput, output_autoregr; gc.collect()\n",
    "\n",
    "\n",
    "    jasper = SentenceTransformer(\"infgrad/jasper_en_vision_language_v1\",\n",
    "        trust_remote_code=True,\n",
    "        device=device,\n",
    "        model_kwargs={\"torch_dtype\":  torch.bfloat16 if device == torch.device('cuda') else torch.float32},\n",
    "    ); jasper.max_seq_length = 1024\n",
    "\n",
    "    jasper_emb = []\n",
    "    for data in text_autoregr:\n",
    "        jasper_emb.append(torch.from_numpy(jasper.encode(data)).to(device))\n",
    "\n",
    "    torch.cuda.empty_cache(); del jasper; gc.collect()\n",
    "    \n",
    "    \n",
    "    final_jasper, paragraphed_jasper = calculate_score(jasper_emb, jasperoutput)\n",
    "    \n",
    "    torch.cuda.empty_cache(); del jasper_emb, jasperoutput; gc.collect()\n",
    "\n",
    "    if final_sonar > 0.4 or final_jasper > 0.4:\n",
    "        torch.save([model.state_dict(), config], '/LCM/Base_LCM_' + str(round(final_sonar, 2)) + '_' + str(round(final_jasper, 2)) + '.pth')\n",
    "\n",
    "    print('Jasper score:', final_jasper, paragraphed_jasper)\n",
    "    wandb.log({'Final jasper': final_jasper, 'Paragraphed jasper': paragraphed_jasper})\n",
    "    \n",
    "    print('Final score:', (final_sonar+final_jasper)/2)\n",
    "    wandb.log({'Final score': (final_sonar+final_jasper)/2})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lcm.models.two_tower_diffusion_lcm.archs import base_lcm_max\n",
    "from lcm.models.two_tower_diffusion_lcm.builder import BaseLCModelBuilder\n",
    "\n",
    "def objective(config, data):\n",
    "    \n",
    "    traindata, valloader, test_data = data\n",
    "    \n",
    "    #model = BaseLCModelBuilder(base_lcm_tuner(config), device=device).build_model(config.sonar_normalizer_name)\n",
    "    model = BaseLCModelBuilder(base_lcm_max(config), device=device).build_model(config.sonar_normalizer_name)\n",
    "    #model = BaseLCModelBuilder(base_lcm_max(config), device=device).build_model()\n",
    "    model_save = model\n",
    "    \n",
    "    optimizer = AdamW(model.parameters(), lr=config.lr, weight_decay=config.weight_decay)\n",
    "\n",
    "    if config.scheduler:\n",
    "        scheduler = CosineAnnealingWarmRestarts(optimizer, T_0=config.T_0)\n",
    "    else:\n",
    "        scheduler = None\n",
    "\n",
    "    val_list=[]\n",
    "    cpt = 1\n",
    "    loss_type=1 #COSSIM:0   MSE:1\n",
    "    patience = 3\n",
    "\n",
    "    checker = 6400/config.batch_size\n",
    "\n",
    "    print('')\n",
    "    \n",
    "    for epoch in itertools.cycle(range(10)):\n",
    "        if epoch!=0:\n",
    "                torch.cuda.empty_cache(); del epoch_traindata, trainloader; gc.collect()\n",
    "        if epoch%10==0:\n",
    "            epoch_traindata = traindata\n",
    "        else:\n",
    "            epoch_traindata = torch.from_numpy(np.load('/LCM/data/'+str(epoch)+'00k.npy'))\n",
    "            \n",
    "        trainloader = DataLoader(epoch_traindata, config.batch_size)\n",
    "        \n",
    "        model.train()\n",
    "\n",
    "        for i, src in enumerate(trainloader):\n",
    "            optimizer.zero_grad()\n",
    "            src = src.to(device)\n",
    "            outputs = model.predict_next_sentence(src[:, :-1])\n",
    "            loss = criterion(outputs, src[:, 1:])[loss_type]\n",
    "            loss.backward()\n",
    "            \n",
    "            if config.clip_grad:\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), 1)\n",
    "            \n",
    "            optimizer.step()\n",
    "            \n",
    "            if scheduler is not None:\n",
    "                scheduler.step()\n",
    "            \n",
    "            if i!=0 and i%checker==0:\n",
    "                val_list, cpt = validation(epoch, cpt, model, valloader, val_list)\n",
    "                \n",
    "                if math.isnan(val_list[-1][1]):\n",
    "                    if len(val_list) > 1:\n",
    "                        test(model_save, test_data, config)\n",
    "                    else:\n",
    "                        wandb.log({'Final score': -1})\n",
    "                    break\n",
    "                \n",
    "                if val_list[0][1]> 0.8: wandb.log({'Cossim': val_list[-1][0], 'MSE': val_list[-1][1]}); test(model_save, test_data, config); break\n",
    "                if cpt==5 and val_list[-1][1]> 0.01: wandb.log({'Cossim': val_list[-1][0], 'MSE': val_list[-1][1]}); test(model_save, test_data, config); break\n",
    "                if cpt==30 and val_list[-1][1]> 5e-5: wandb.log({'Cossim': val_list[-1][0], 'MSE': val_list[-1][1]}); test(model_save, test_data, config); break\n",
    "                \n",
    "                #early stopping code:\n",
    "                if len(val_list) >= 2:\n",
    "                    loss_list = [x[loss_type] for x in val_list]\n",
    "                    if loss_list[-1] < min(loss_list[:-1]):#it performs better, we save the model\n",
    "                        model_save = model\n",
    "                    elif (len(loss_list) - loss_list.index(min(loss_list))) > patience: #no better model in the last epochs\n",
    "                        print('Best: Part', cpt-1-patience, 'Cossim', val_list[-1-patience][0], 'MSE', val_list[-1-patience][1])\n",
    "                        wandb.log({'Cossim': val_list[-1-patience][0], 'MSE': val_list[-1-patience][1]})\n",
    "                        torch.cuda.empty_cache(); del src, outputs, model, optimizer, valloader, trainloader, epoch_traindata, traindata, loss; gc.collect()\n",
    "                        \n",
    "                        #compute Final score\n",
    "                        test(model_save, test_data, config)\n",
    "                        break\n",
    "        else:\n",
    "            continue\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sweep_conf = {\n",
    "    \"method\": \"bayes\",\n",
    "    \"metric\": {\"goal\": \"maximize\", \"name\": \"Final score\"},\n",
    "    \"parameters\": {\n",
    "        \"batch_size\": {\"values\": [8, 16, 32, 64]},\n",
    "        \"scheduler\": {\"values\": [True, False]},\n",
    "        \"T_0\": {\"min\": 10, \"max\": 20000},\n",
    "        \"lr\": {\"distribution\": \"log_uniform_values\", \"min\": 0.00001, \"max\": 0.1},\n",
    "        \"weight_decay\": {\"distribution\": \"log_uniform_values\", \"min\": 0.00001, \"max\": 0.1},\n",
    "        \"clip_grad\": {\"values\": [True, False]},\n",
    "        \n",
    "        #\"model_dim\": {\"values\": [512, 1024, 2048]},\n",
    "        \"model_dim\": {\"values\": [2048]},\n",
    "        #\"num_attn_heads\": {\"values\": [16]},\n",
    "        \n",
    "        #\"sonar_normalizer_name\": {\"values\": [\"dummy_sonar_normalizer\", \"layernorm\", None]},\n",
    "\n",
    "        #\"frontend_dropout_p\": {\"min\": 0.0, \"max\": 0.5},\n",
    "        #\"frontend_pre_linear_init_fn\": {\"values\": ['xavier', 'sonar', 'zero', 'trunc_normal', 'kaiming_uniform', 'none']},\n",
    "        #\"frontend_scale_embeddings\": {\"values\": [True, False]},\n",
    "        #\"frontend_weight_normalization\": {\"values\": [True, False]},\n",
    "\n",
    "        #\"lcm_final_dropout_p\": {\"min\": 0.0, \"max\": 0.5},\n",
    "        #\"lcm_attention_dropout_p\": {\"min\": 0.0, \"max\": 0.5},\n",
    "        #\"lcm_dropout_p\": {\"min\": 0.0, \"max\": 0.5},\n",
    "        #\"lcm_ffn_inner_dim\": {\"values\": [1, 2, 4]},\n",
    "        \"lcm_ffn_inner_dim\": {\"values\": [2]},\n",
    "        #\"lcm_num_layers\": {\"values\": [2, 8, 16, 24, 32]},\n",
    "        #\"lcm_num_layers\": {\"values\": [2, 8, 14, 24, 32]},\n",
    "        \"lcm_num_layers\": {\"values\": [18]},\n",
    "        #\"lcm_pos_embedding_style\": {\"values\": [\"rope\", \"sine\", \"learned\", \"none\"]},\n",
    "        #\"lcm_use_swiglu\": {\"values\": [True, False]},\n",
    "        #\"lcm_ffn_inner_activation_name\": {\"values\": [\"relu\", \"tanh\", \"elu\", \"leaky_relu\", \"prelu\", \"selu\", \"gelu\", \"silu\", \"softsign\", \"sigmoid\", \"hardsigmoid\", None]},\n",
    "        #\"lcm_ffn_inner_activation_name\": {\"values\": [\"relu\", \"tanh\", \"elu\", \"leaky_relu\", \"selu\", \"gelu\", \"silu\", \"softsign\", \"sigmoid\", \"hardsigmoid\", None]},\n",
    "        #\"lcm_layer_normalization_style\": {\"values\": [\"standard\", \"fp32\", \"rms\", \"unit\"]},\n",
    "        #\"lcm_norm_order_style\": {\"values\": ['pre', 'post', 'normformer']},\n",
    "        #\"lcm_final_norm_order_style\": {\"values\": ['pre', 'post', 'normformer']},\n",
    "        #\"lcm_enable_qk_layernorm\": {\"values\": [True, False]},\n",
    "        #\"lcm_mha_qkv_weight_normalization\": {\"values\": [True, False]},\n",
    "        #\"lcm_mha_output_weight_normalization\": {\"values\": [True, False]},\n",
    "        #\"lcm_mha_output_proj_bias\": {\"values\": [True, False]},\n",
    "        #\"lcm_attention_output_init_fn\": {\"values\": ['xavier', 'sonar', 'zero', 'trunc_normal', 'kaiming_uniform', 'none']},\n",
    "\n",
    "        #\"postnet_dropout_p\": {\"min\": 0.0, \"max\": 0.5},\n",
    "        #\"postnet_linear_init_fn\": {\"values\": ['xavier', 'sonar', 'zero', 'trunc_normal', 'kaiming_uniform', 'none']},\n",
    "        #\"postnet_weight_normalization\": {\"values\": [True, False]},\n",
    "        #\"postnet_layer_normalization_style\": {\"values\": [\"standard\", \"fp32\", \"rms\", \"unit\"]},\n",
    "        #\"postnet_activation_name\": {\"values\": [\"relu\", \"tanh\", \"elu\", \"leaky_relu\", \"prelu\", \"selu\", \"gelu\", \"silu\", \"softsign\", \"sigmoid\", \"hardsigmoid\", None]},\n",
    "    },\n",
    "}\n",
    "\n",
    "np_data = np.load('/LCM/data/100k_1k_0.npz')\n",
    "traindata = torch.from_numpy(np_data['train'])\n",
    "valloader = DataLoader(torch.from_numpy(np_data['val']), batch_size=64)\n",
    "test_data = torch.load('/LCM/data/test_sonarprompt_sonaroutput_jasperoutput.pth')\n",
    "torch.cuda.empty_cache(); del np_data; gc.collect()\n",
    "\n",
    "\n",
    "#os.environ[\"WANDB_SILENT\"] = \"true\"\n",
    "\n",
    "def main():\n",
    "    wandb.init()\n",
    "    try:\n",
    "        objective(wandb.config, [traindata, valloader, test_data])\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        traceback.print_exc()\n",
    "        wandb.log({'Final score': -1})\n",
    "    \n",
    "sweep_id = wandb.sweep(sweep_conf, project=\"LCM\")\n",
    "\n",
    "wandb.agent(sweep_id, function=main)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "large-concept-model-py3.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
